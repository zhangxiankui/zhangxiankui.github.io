---
layout: post
title:  "Hbase相关业务系统性能优化"
categories: 大数据 性能优化
tags:  Hbase 问题处理
author: zhangxiankui
---

* content
{:toc}


## 一、问题描述：
某支付数据系统后台提供数据查询功能，通过查询hbase表数据展示到页面，形成当天的支付单量、转化率等信息，但是生产环境查询页面需要6-7秒才能展示页面，产品提出优化需求
	
## 二、问题定位：
- 问题可能出在两个地方，第一前端页面加载组件慢导致页面返现速度慢，第二后台查询时间长导致
- 这里开始设想是通过日志方式来进行前后端耗时的排查，但是后来发现公司中有对应监控工具，专门用来监控对应系统的接口，其中包括一些
响应时间TP99，TP999，以及可以查询具体的接口日志用来做线上问题分析等（话说一般大厂必备的工具，之前在小公司没见识过。。很好用），然后通过URL监控可以发现，
后台查询的接口耗时的TP99大概在6.5秒
- 这样一来其实不必再去考虑前端加载耗时的问题，问题可以基本确定出现在是后台查询这块
- 后台代码只是很简单的从Hbase表中取数据，并且封装对象返回，直觉第一反应是Hbase的表出现了问题，通过打点日志，果然是hbase查询数据返回耗时太长，这时候就怀疑应该是hbase的rowkey设计出现了问题
- 通过查看数据文档得知hbase设计的rowkey并不是以时间来进行加盐的（hbase的rowkey生成策略以及加盐自行百度哈），但是后台查询时候会去查询当前时间和对比期两天时间的数据，如果不按照时间加盐的话，
就会导致查询的时候去扫描hbase服务器的所有分区（有点相当于关系型数据库的全表扫描的概念），可想而知这样的查询效率会很低，而且随着数据的增长，必定还会增加查询的时间，显然这地方是非改不可的
- 但是熟悉Hbase的同学都知道，一旦用时间来进行加盐的话，势必会导致hbase的单点过热问题，通过分析对rowkey和value大小、目前系统该表对应业务TPS的考虑，综合计算得出该表每天产生的
数据量是30M（由于是汇总表，所以数据量较小）
		
## 三、问题解决
- 综合以上，但是考虑到后台系统查询需求少，并且通过hash算法可以使得一个月的数据平均分布在不同的分区上，另外数据系统的主要功能还是集中在短信只能报警模块，
所以决定对rowkey的生成策略做改动，通过时间来加盐，这样同一天数据就会落到同一个分区；这样做的好处是，不管以后该表的数据怎样的增长，查询速度都不会改变
- 优化上线之后，发现该接口TP99在0.5秒左右，持续观察1个月，并未出现查询慢问题，问题解决！